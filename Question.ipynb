{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Cloud-Driven Loan Default Predictor using Machine Learning***\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Please run the below cell to import libraries:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import statements here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report \n",
    "from sklearn import datasets\n",
    "\n",
    "import warnings\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task I - Data Loading \n",
    "\n",
    "**Instructions:**\n",
    "- Build the S3 path for the dataset `loan_cleaned_data.csv` using string formatting to concatenate the bucket name, folder name and file key i.e the name of the dataset. \n",
    "    - Note: Bucket name - ` loan_dataXYZXYZ` (XYZXYZ can be any random integers) & Folder name - ` loan_cleaned_data`.    \n",
    "- Load the dataset into a pandas DataFrame. \n",
    "\n",
    "\n",
    "**Hints:**\n",
    "- Sample S3 URI - “s3://bucket_name/folder_name/file_name.csv” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import the dataset from S3\n",
    "bucket= None\n",
    "folder_name = None\n",
    "data_key = None\n",
    "data_location = \"S3 URI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Load the dataset\n",
    "\n",
    "data= pd.read_csv(data_location)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task II - Feature Engineering\n",
    "\n",
    "**Instructions:**\n",
    "- Convert the values in the categorical column `purpose` into numerical format using **One-hot Encoding**. The datatype of the new columns should be *int*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Store the updated dataframe below\n",
    "\n",
    "data = pd.get_dummies(data,columns=['purpose'],dtype=int)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task III - Data Preprocessing\n",
    "\n",
    "**Instructions:**\n",
    "- Inspect the target column `not_fully_paid` and identify the count of records belonging to the two classes.\n",
    "- Filter out the majority and minority classes and store them separately.\n",
    "- Handle the data imbalance by oversampling the minority class using the **resample** method so that the final count of records in both the classes becomes equal. Store the result in the variable *df_minority_upsampled*.\n",
    "- Concatenate the upsampled minority data with the majority and assign the result to the new dataframe *df*. \n",
    "- Inspect the target column of the new dataframe to verify that the data is balanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['not_fully_paid'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = data[data['not_fully_paid'] == 0]\n",
    "df_minority = data[data['not_fully_paid'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the imbalanced data using resample method and oversample the minority class\n",
    "df_minority_upsampled = resample(df_minority, replace=True,n_samples=df_majority.shape[0],random_state=42)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the upsampled data records with the majority class records and shuffle the resultant dataframe\n",
    "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "#Optional\n",
    "print(df_balanced['not_fully_paid'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task IV - Model Training\n",
    "\n",
    "**Instructions:**\n",
    "- Drop the columns `sl_no` and `not_fully_paid` and create a dataframe of independent variables named *X*. Filter the dependent variable and store it in *y*.\n",
    "- Split the data into training and test sets using **60:40** ratio. Use a random state equal to **42**.\n",
    "- Train a **Random Forest Classifier** model called *rf* using the training data. Use a random state equal to **42**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y data for train-test split\n",
    "\n",
    "X = df_balanced.drop(['sl_no', 'not_fully_paid'], axis=1)\n",
    "y = df_balanced['not_fully_paid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest Classifier model\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task V - Model Evaluation\n",
    "\n",
    "**Instructions:**\n",
    "- Predict using the trained **Random Forest Classifier** model *rf* on the test data *X_test*.\n",
    "- Evaluate the predictions by comparing it with the actual test data *y_test*. \n",
    "- Print the classification report to determine the evaluation metric scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the trained Random Forest Classifier model\n",
    "\n",
    "y_pred = rf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report \n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task VI - Saving the Model to AWS S3 \n",
    "\n",
    "**Instructions:** \n",
    "- Serialize the trained Random Forest model using `joblib`. \n",
    "- Initialize the S3 client using the `boto3` library. \n",
    "- Save the serialized model to a temporary file using `tempfile`. \n",
    "- Upload the model file to the specified S3 bucket named `loan-data`. \n",
    "- Ensure the model is saved as `model.pkl` in the S3 bucket. \n",
    "\n",
    "**Hints:**\n",
    "- Temporary files in Python can be managed using `tempfile.TemporaryFile().` \n",
    "- Use `joblib.dump()` for saving the model. \n",
    "- We can push objects into S3 using `.put_object(...) method with necessary parameters available under boto3. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Uploading the model data to S3 bucket\n",
    "import tempfile\n",
    "import boto3\n",
    "import joblib\n",
    "\n",
    "BUCKET_NAME = \"Loan_data\"\n",
    "\n",
    "# intialize s3 client to save model\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# name to save model as in s3\n",
    "model_name = \"model.pkl\"\n",
    "\n",
    "# save to s3 - make necessary changes to the function\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    joblib.dump(rf, fp) # Replace with appropriate field\n",
    "    fp.seek(0)\n",
    "    s3_client.put_object( # Use appropriate function name\n",
    "        Body=fp.read(), \n",
    "        Bucket=BUCKET_NAME,\n",
    "        Key=model_name\n",
    "    )\n",
    "\n",
    "print(f'Model saved to s3 as: {model_name}')\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
